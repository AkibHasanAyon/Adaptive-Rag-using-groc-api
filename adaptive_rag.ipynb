{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0231ab8",
   "metadata": {},
   "source": [
    "### Import Libraries & Set API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06ca9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key from environment variable\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Ensure it's set\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"Missing GROQ_API_KEY in .env file or environment variables.\")\n",
    "\n",
    "# Initialize Groq API client\n",
    "client = Groq(api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46634fe4",
   "metadata": {},
   "source": [
    "### Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f39859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = \"\"\n",
    "    for page in doc:\n",
    "        all_text += page.get_text(\"text\")\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08208ab8",
   "metadata": {},
   "source": [
    "### Chunk the Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d16f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d5abbc",
   "metadata": {},
   "source": [
    "### Simple Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee82131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "\n",
    "    def add_item(self, text, embedding):\n",
    "        self.texts.append(text)\n",
    "        self.vectors.append(np.array(embedding))\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "\n",
    "        query_vec = np.array(query_embedding)\n",
    "        similarities = [\n",
    "            (i, np.dot(query_vec, vec) / (np.linalg.norm(query_vec) * np.linalg.norm(vec)))\n",
    "            for i, vec in enumerate(self.vectors)\n",
    "        ]\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [self.texts[i] for i, _ in similarities[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d45c7",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cb10f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of text chunks using batch processing.\n",
    "\n",
    "    Args:\n",
    "        text_list (List[str]): A list of text chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[np.ndarray]: List of embedding vectors.\n",
    "    \"\"\"\n",
    "    return embedding_model.encode(text_list, convert_to_numpy=True, batch_size=16, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210095ad",
   "metadata": {},
   "source": [
    "### Groq Chat Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7459537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groq_chat(system_prompt, user_prompt, model=\"llama3-70b-8192\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd3a53",
   "metadata": {},
   "source": [
    "### Document Processing with Batch Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bfd7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF, chunks it, and creates batch embeddings.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        chunk_size (int): Number of characters per chunk.\n",
    "        chunk_overlap (int): Overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], SimpleVectorStore]: Chunked texts and vector store.\n",
    "    \"\"\"\n",
    "    print(\"üìÑ Extracting text from PDF...\")\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    print(\"‚úÇÔ∏è Chunking text...\")\n",
    "    chunks = chunk_text(text, chunk_size, chunk_overlap)\n",
    "    print(f\"‚úÖ Total Chunks: {len(chunks)}\")\n",
    "\n",
    "    print(\"üîó Generating embeddings (batched)...\")\n",
    "    embeddings = get_embeddings(chunks)  # Batch version from Segment 6\n",
    "\n",
    "    print(\"üì¶ Storing embeddings in vector store...\")\n",
    "    store = SimpleVectorStore()\n",
    "    for chunk, vector in zip(chunks, embeddings):\n",
    "        store.add_item(chunk, vector)\n",
    "\n",
    "    print(f\"‚úÖ Added {len(store.texts)} chunks to vector store\")\n",
    "    return chunks, store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4ff38",
   "metadata": {},
   "source": [
    "### Factual Retrieval Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48a5ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factual_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Uses Groq to enhance a factual query, generate embedding,\n",
    "    and retrieve top-k similar documents.\n",
    "\n",
    "    Args:\n",
    "        query (str): Original user query.\n",
    "        vector_store (SimpleVectorStore): Chunk store with embeddings.\n",
    "        k (int): Number of results to return.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Top-k most relevant document chunks.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Executing Factual Retrieval for: \\\"{query}\\\"\")\n",
    "\n",
    "    # Enhance query using Groq\n",
    "    system_prompt = \"\"\"You are an expert at refining factual search queries.\n",
    "Make the user's question more specific, clear, and optimized for information retrieval.\n",
    "Do NOT add opinions or unrelated info. Return only the enhanced query.\"\"\"\n",
    "    \n",
    "    enhanced_query = groq_chat(system_prompt, query)\n",
    "    print(f\"‚úÖ Enhanced Query: {enhanced_query}\")\n",
    "\n",
    "    # Generate embedding for the enhanced query\n",
    "    query_embedding = get_embedding(enhanced_query)\n",
    "\n",
    "    # Perform similarity search\n",
    "    top_chunks = vector_store.similarity_search(query_embedding, k=k)\n",
    "\n",
    "    print(f\"üìÑ Retrieved Top {k} Relevant Chunks.\")\n",
    "    return top_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e7ac7",
   "metadata": {},
   "source": [
    "### Analytical Retrieval Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4075269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieves diverse and comprehensive results by generating sub-questions.\n",
    "\n",
    "    Args:\n",
    "        query (str): Original analytical question.\n",
    "        vector_store (SimpleVectorStore): Vector store with document chunks.\n",
    "        k (int): Number of final documents to return.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Top-k diverse document chunks.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä Executing Analytical Retrieval for: \\\"{query}\\\"\")\n",
    "\n",
    "    # Step 1: Generate sub-questions using Groq\n",
    "    system_prompt = \"\"\"You are an expert at breaking down complex analytical queries.\n",
    "Generate exactly 3 sub-questions that explore different dimensions of the main query.\n",
    "Return each sub-question on a new line without numbering or explanation.\"\"\"\n",
    "\n",
    "    response = groq_chat(system_prompt, query)\n",
    "    sub_questions = [q.strip() for q in response.split(\"\\n\") if q.strip()]\n",
    "\n",
    "    print(\"‚úÖ Sub-questions generated:\")\n",
    "    for i, sq in enumerate(sub_questions, 1):\n",
    "        print(f\"  {i}. {sq}\")\n",
    "\n",
    "    # Step 2: For each sub-question ‚Üí embedding + similarity search\n",
    "    results = []\n",
    "    seen_texts = set()\n",
    "\n",
    "    for sub_query in sub_questions:\n",
    "        sub_embedding = get_embedding(sub_query)\n",
    "        sub_results = vector_store.similarity_search(sub_embedding, k=2)\n",
    "\n",
    "        for chunk in sub_results:\n",
    "            if chunk not in seen_texts:\n",
    "                seen_texts.add(chunk)\n",
    "                results.append(chunk)\n",
    "\n",
    "    # Step 3: Fallback if less than k results\n",
    "    if len(results) < k:\n",
    "        print(\"‚ö†Ô∏è Not enough unique chunks from sub-questions. Fetching directly from main query.\")\n",
    "        main_embedding = get_embedding(query)\n",
    "        fallback_chunks = vector_store.similarity_search(main_embedding, k=k)\n",
    "\n",
    "        for chunk in fallback_chunks:\n",
    "            if chunk not in seen_texts and len(results) < k:\n",
    "                seen_texts.add(chunk)\n",
    "                results.append(chunk)\n",
    "\n",
    "    print(f\"üìö Returning {len(results)} analytical chunks.\")\n",
    "    return results[:k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac6951",
   "metadata": {},
   "source": [
    "### Opinion Retrieval Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b82ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieves a diverse set of perspectives on an opinion-based query.\n",
    "\n",
    "    Args:\n",
    "        query (str): Original opinion-based query.\n",
    "        vector_store (SimpleVectorStore): Vector store with document chunks.\n",
    "        k (int): Number of final documents to return.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Top-k opinion chunks representing different perspectives.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüó£Ô∏è Executing Opinion Retrieval for: \\\"{query}\\\"\")\n",
    "\n",
    "    # Step 1: Generate perspective angles using Groq\n",
    "    system_prompt = \"\"\"You are an expert at identifying diverse viewpoints.\n",
    "For the given opinion-based query, generate exactly 3 different perspective angles \n",
    "people might have. Each one should reflect a distinct viewpoint.\n",
    "\n",
    "Return one viewpoint per line with NO explanation or numbering.\"\"\"\n",
    "\n",
    "    response = groq_chat(system_prompt, query)\n",
    "    perspectives = [p.strip() for p in response.split(\"\\n\") if p.strip()]\n",
    "\n",
    "    print(\"‚úÖ Identified perspectives:\")\n",
    "    for i, p in enumerate(perspectives, 1):\n",
    "        print(f\"  {i}. {p}\")\n",
    "\n",
    "    # Step 2: Combine query with each perspective and embed + search\n",
    "    results = []\n",
    "    seen_texts = set()\n",
    "\n",
    "    for perspective in perspectives:\n",
    "        perspective_query = f\"{query} | {perspective}\"\n",
    "        emb = get_embedding(perspective_query)\n",
    "        similar_chunks = vector_store.similarity_search(emb, k=2)\n",
    "\n",
    "        for chunk in similar_chunks:\n",
    "            if chunk not in seen_texts:\n",
    "                seen_texts.add(chunk)\n",
    "                results.append(chunk)\n",
    "\n",
    "    # Step 3: Fallback to generic similarity search if needed\n",
    "    if len(results) < k:\n",
    "        print(\"‚ö†Ô∏è Not enough unique perspective chunks. Adding fallback...\")\n",
    "        base_emb = get_embedding(query)\n",
    "        fallback_chunks = vector_store.similarity_search(base_emb, k=k)\n",
    "\n",
    "        for chunk in fallback_chunks:\n",
    "            if chunk not in seen_texts and len(results) < k:\n",
    "                seen_texts.add(chunk)\n",
    "                results.append(chunk)\n",
    "\n",
    "    print(f\"üìö Returning {len(results)} opinion chunks.\")\n",
    "    return results[:k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42348e02",
   "metadata": {},
   "source": [
    "### Dispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0634cf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_retrieval(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Determines query type and dynamically routes to the appropriate retrieval strategy.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's input query.\n",
    "        vector_store (SimpleVectorStore): Precomputed document chunks and embeddings.\n",
    "        k (int): Number of results to retrieve.\n",
    "        user_context (str, optional): Extra context for contextual queries.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[str]]: Query type and top-k retrieved document chunks.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüß† Classifying query: \\\"{query}\\\"\")\n",
    "    \n",
    "    # Step 1: Classify query type via Groq\n",
    "    system_prompt = \"\"\"You are an expert at query classification.\n",
    "Classify the input into one of the following categories:\n",
    "- Factual\n",
    "- Analytical\n",
    "- Opinion\n",
    "- Contextual\n",
    "\n",
    "Return ONLY the category name with no explanation.\"\"\"\n",
    "    \n",
    "    query_type = groq_chat(system_prompt, query)\n",
    "    print(f\"‚úÖ Query classified as: {query_type}\")\n",
    "\n",
    "    # Step 2: Route to the correct strategy\n",
    "    if query_type.lower() == \"factual\":\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type.lower() == \"analytical\":\n",
    "        results = analytical_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type.lower() == \"opinion\":\n",
    "        results = opinion_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type.lower() == \"contextual\":\n",
    "        print(\"‚ö†Ô∏è Contextual strategy not yet implemented. Falling back to factual.\")\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Unknown type. Defaulting to factual.\")\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "\n",
    "    return query_type, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4579dcc7",
   "metadata": {},
   "source": [
    "### Response Generation using Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ea6d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, retrieved_chunks, query_type=\"Factual\", model=\"llama3-70b-8192\"):\n",
    "    \"\"\"\n",
    "    Generates a final response based on the query, retrieved documents, and query type.\n",
    "\n",
    "    Args:\n",
    "        query (str): The original user query.\n",
    "        retrieved_chunks (List[str]): Top-k relevant document chunks.\n",
    "        query_type (str): Type of the query (Factual, Analytical, etc.)\n",
    "        model (str): Groq model to use.\n",
    "\n",
    "    Returns:\n",
    "        str: Final natural language response.\n",
    "    \"\"\"\n",
    "    # Combine the top chunks into a single context string\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # Use different instructions based on query type\n",
    "    system_prompts = {\n",
    "        \"Factual\": \"\"\"You are a helpful assistant. Provide accurate, clear, and precise answers.\n",
    "Use only the context provided below. If the answer isn't in the context, say so.\"\"\",\n",
    "        \"Analytical\": \"\"\"You are a helpful assistant. Provide a detailed, well-structured analysis of the topic\n",
    "using the provided context. Consider multiple dimensions where appropriate.\"\"\",\n",
    "        \"Opinion\": \"\"\"You are a helpful assistant. Summarize multiple viewpoints on the topic using the context.\n",
    "Present diverse opinions neutrally and fairly.\"\"\",\n",
    "        \"Contextual\": \"\"\"You are a helpful assistant. Answer the question based on both the query and the user's context\n",
    "as reflected in the provided information.\"\"\",\n",
    "    }\n",
    "\n",
    "    system_prompt = system_prompts.get(query_type, system_prompts[\"Factual\"])\n",
    "\n",
    "    # User message combining context and query\n",
    "    user_prompt = f\"\"\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "\n",
    "    # Generate the answer using Groq\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67c0a811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Extracting text from PDF...\n",
      "‚úÇÔ∏è Chunking text...\n",
      "‚úÖ Total Chunks: 37\n",
      "üîó Generating embeddings (batched)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544e6e0ef43a45afa32ce40740064d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Storing embeddings in vector store...\n",
      "‚úÖ Added 37 chunks to vector store\n",
      "\n",
      "üß† Classifying query: \"What is the file about?\"\n",
      "‚úÖ Query classified as: Contextual\n",
      "‚ö†Ô∏è Contextual strategy not yet implemented. Falling back to factual.\n",
      "\n",
      "üîç Executing Factual Retrieval for: \"What is the file about?\"\n",
      "‚úÖ Enhanced Query: Which specific file are you referring to? Please provide more context or details. Here's a refined query:\n",
      "\n",
      "\"What is the content or purpose of [file name/file type/file location]?\"\n",
      "üìÑ Retrieved Top 4 Relevant Chunks.\n",
      "\n",
      "üß† Query Type: Contextual\n",
      "\n",
      "üí¨ Final Answer:\n",
      "The file appears to be a research paper or article about liver disease prediction and classification using machine learning techniques. It discusses the use of different classification algorithms, such as Decision Tree, Random Forest, k-Nearest Neighbors, and Support Vector Machine, to improve the accuracy of liver disease prediction models.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 1: Load and Process Your PDF File\n",
    "pdf_path = \"A Comprehensive Analysis of Liver Disease Detection Using Advanced Machine Learning Algorithms.pdf\"  # üîÅ Replace with your actual PDF file path\n",
    "chunks, vector_store = process_document(pdf_path)\n",
    "\n",
    "# ‚úÖ Step 2: Enter Your Query\n",
    "query = \"What is the file about?\"  # üîÅ Replace with your own query\n",
    "\n",
    "# ‚úÖ Step 3: Adaptive Retrieval Based on Query Type\n",
    "query_type, top_chunks = adaptive_retrieval(query, vector_store, k=4)\n",
    "\n",
    "# ‚úÖ Step 4: Generate Final Answer Using Groq\n",
    "final_answer = generate_response(query, top_chunks, query_type)\n",
    "\n",
    "# ‚úÖ Step 5: Display Results\n",
    "print(f\"\\nüß† Query Type: {query_type}\")\n",
    "print(f\"\\nüí¨ Final Answer:\\n{final_answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb7842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
